# ğŸ§ª StratÃ©gie de Tests - Rakuten Product Classifier

## Executive Summary

Ce document dÃ©finit une stratÃ©gie de tests **enterprise-grade** pour garantir la qualitÃ©, la fiabilitÃ© et la maintenabilitÃ© de l'application Rakuten Product Classifier.

**Objectif de couverture**: â‰¥ 85% du code critique

---

## ğŸ“Š Pyramide des Tests

```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   E2E Tests     â”‚  â† 5% (Playwright)
                    â”‚   (UI/Browser)  â”‚
                    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
                    â”‚  Integration    â”‚  â† 20% (Streamlit + ML)
                    â”‚     Tests       â”‚
                    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
                    â”‚   ML-Specific   â”‚  â† 25% (Model Quality)
                    â”‚     Tests       â”‚
                    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
                    â”‚                 â”‚
                    â”‚   Unit Tests    â”‚  â† 50% (Functions/Classes)
                    â”‚                 â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ¯ 1. TESTS UNITAIRES (Unit Tests)

### 1.1 Module `utils/mock_classifier.py`

| Test ID | Fonction testÃ©e | Description | PrioritÃ© |
|---------|----------------|-------------|----------|
| UT-MC-001 | `DemoClassifier.__init__` | Initialisation avec/sans config | HIGH |
| UT-MC-002 | `DemoClassifier.predict(text=...)` | PrÃ©diction texte seul | HIGH |
| UT-MC-003 | `DemoClassifier.predict(image=...)` | PrÃ©diction image seule | HIGH |
| UT-MC-004 | `DemoClassifier.predict(text, image)` | PrÃ©diction multimodale | HIGH |
| UT-MC-005 | `ClassificationResult` | Dataclass attributes | MEDIUM |
| UT-MC-006 | `_generate_predictions` | Distribution probabilitÃ©s | HIGH |
| UT-MC-007 | `ModelConfig` | Validation dataclass | MEDIUM |
| UT-MC-008 | `TEXT_MODELS` | 3 modÃ¨les texte prÃ©sents | HIGH |
| UT-MC-009 | `IMAGE_MODELS` | 3 modÃ¨les image prÃ©sents | HIGH |
| UT-MC-010 | `MultiModelClassifier` | PrÃ©dictions multi-modÃ¨les | HIGH |

**Assertions clÃ©s**:
```python
# UT-MC-002: PrÃ©diction texte
def test_predict_text_returns_valid_result():
    clf = DemoClassifier()
    result = clf.predict(text="iPhone 15 Pro Max")

    assert result is not None
    assert result.category in CATEGORY_MAPPING.keys()
    assert 0.0 <= result.confidence <= 1.0
    assert len(result.top_k_predictions) == 5
    assert sum(p[1] for p in result.top_k_predictions) <= 1.0
    assert result.source in ["mock_text", "text", "demo"]
```

### 1.2 Module `utils/category_mapping.py`

| Test ID | Fonction testÃ©e | Description | PrioritÃ© |
|---------|----------------|-------------|----------|
| UT-CM-001 | `CATEGORY_MAPPING` | 27 catÃ©gories prÃ©sentes | HIGH |
| UT-CM-002 | `get_category_info(code)` | Retourne (name, full, emoji) | HIGH |
| UT-CM-003 | `get_category_info(invalid)` | Gestion code invalide | MEDIUM |
| UT-CM-004 | `get_category_emoji(code)` | Retourne emoji correct | LOW |
| UT-CM-005 | `get_all_categories()` | Retourne dict complet | MEDIUM |

**Assertions clÃ©s**:
```python
# UT-CM-001: VÃ©rification 27 catÃ©gories
def test_category_mapping_has_27_categories():
    assert len(CATEGORY_MAPPING) == 27

# UT-CM-002: Structure correcte
def test_get_category_info_returns_tuple():
    name, full, emoji = get_category_info("2583")
    assert isinstance(name, str)
    assert isinstance(full, str)
    assert len(emoji) > 0  # Emoji prÃ©sent
```

### 1.3 Module `utils/preprocessing.py`

| Test ID | Fonction testÃ©e | Description | PrioritÃ© |
|---------|----------------|-------------|----------|
| UT-PP-001 | `preprocess_product_text` | Nettoyage HTML | HIGH |
| UT-PP-002 | `preprocess_product_text` | Combinaison designation+description | HIGH |
| UT-PP-003 | `preprocess_product_text` | Gestion texte vide | MEDIUM |
| UT-PP-004 | `preprocess_product_text` | CaractÃ¨res spÃ©ciaux | MEDIUM |
| UT-PP-005 | `validate_text_input` | Validation longueur | HIGH |
| UT-PP-006 | `clean_html` | Suppression balises | HIGH |

**Cas de test**:
```python
@pytest.mark.parametrize("input_text,expected_contains", [
    ("<p>Test</p>", "Test"),           # HTML removal
    ("  spaces  ", "spaces"),          # Trim
    ("UPPERCASE", "uppercase"),        # Lowercase (si applicable)
    ("", ""),                          # Empty string
    (None, ""),                        # None handling
])
def test_preprocess_handles_various_inputs(input_text, expected_contains):
    result = preprocess_product_text(input_text, "")
    if expected_contains:
        assert expected_contains in result.lower()
```

### 1.4 Module `utils/image_utils.py`

| Test ID | Fonction testÃ©e | Description | PrioritÃ© |
|---------|----------------|-------------|----------|
| UT-IU-001 | `load_image_from_upload` | Chargement JPEG | HIGH |
| UT-IU-002 | `load_image_from_upload` | Chargement PNG | HIGH |
| UT-IU-003 | `validate_image` | Image valide | HIGH |
| UT-IU-004 | `validate_image` | Image trop petite | MEDIUM |
| UT-IU-005 | `validate_image` | Image trop grande | MEDIUM |
| UT-IU-006 | `get_image_info` | MÃ©tadonnÃ©es correctes | LOW |
| UT-IU-007 | `resize_image` | Redimensionnement | MEDIUM |

### 1.5 Module `utils/data_loader.py`

| Test ID | Fonction testÃ©e | Description | PrioritÃ© |
|---------|----------------|-------------|----------|
| UT-DL-001 | `is_data_available` | DÃ©tection donnÃ©es rÃ©elles | HIGH |
| UT-DL-002 | `get_dataset_summary` | Retourne dict valide | HIGH |
| UT-DL-003 | `get_category_distribution` | DataFrame correct | HIGH |
| UT-DL-004 | `get_text_statistics` | Stats texte | MEDIUM |
| UT-DL-005 | `load_training_data` | Fallback mode dÃ©mo | HIGH |

### 1.6 Module `config.py`

| Test ID | Fonction testÃ©e | Description | PrioritÃ© |
|---------|----------------|-------------|----------|
| UT-CF-001 | `APP_CONFIG` | ClÃ©s requises prÃ©sentes | HIGH |
| UT-CF-002 | `MODEL_CONFIG` | Configuration modÃ¨le | HIGH |
| UT-CF-003 | `THEME` | Couleurs dÃ©finies | LOW |
| UT-CF-004 | `ASSETS_DIR` | Path existe | MEDIUM |

---

## ğŸ”— 2. TESTS D'INTÃ‰GRATION (Integration Tests)

### 2.1 Tests de chargement des pages

| Test ID | Page testÃ©e | Description | PrioritÃ© |
|---------|-------------|-------------|----------|
| IT-PG-001 | `app.py` | Page accueil charge sans erreur | CRITICAL |
| IT-PG-002 | `1_ğŸ“Š_DonnÃ©es.py` | Page donnÃ©es charge | CRITICAL |
| IT-PG-003 | `2_âš™ï¸_Preprocessing.py` | Page preprocessing charge | CRITICAL |
| IT-PG-004 | `3_ğŸ§ _ModÃ¨les.py` | Page modÃ¨les charge | CRITICAL |
| IT-PG-005 | `4_ğŸ”_DÃ©mo.py` | Page dÃ©mo charge | CRITICAL |
| IT-PG-006 | `5_ğŸ“ˆ_Performance.py` | Page performance charge | CRITICAL |
| IT-PG-007 | `6_ğŸ’¡_Conclusions.py` | Page conclusions charge | CRITICAL |

**ImplÃ©mentation avec `streamlit.testing`**:
```python
from streamlit.testing.v1 import AppTest

def test_home_page_loads():
    """Test que la page d'accueil charge sans erreur."""
    at = AppTest.from_file("app.py")
    at.run(timeout=30)

    assert not at.exception
    assert "Rakuten" in at.markdown[0].value

def test_demo_page_classification_flow():
    """Test du flow complet de classification."""
    at = AppTest.from_file("pages/4_ğŸ”_DÃ©mo.py")
    at.run()

    # Simuler saisie texte
    at.text_input[0].set_value("iPhone 15 Pro Max")
    at.text_area[0].set_value("Smartphone Apple derniÃ¨re gÃ©nÃ©ration")
    at.button[0].click()
    at.run()

    assert not at.exception
    # VÃ©rifier qu'un rÃ©sultat est affichÃ©
    assert any("CatÃ©gorie" in str(m.value) for m in at.markdown)
```

### 2.2 Tests de Session State

| Test ID | Description | PrioritÃ© |
|---------|-------------|----------|
| IT-SS-001 | Persistance du classifier entre pages | HIGH |
| IT-SS-002 | Historique des classifications | HIGH |
| IT-SS-003 | SÃ©lection de modÃ¨le persiste | HIGH |
| IT-SS-004 | Reset session fonctionne | MEDIUM |

### 2.3 Tests de Navigation

| Test ID | Description | PrioritÃ© |
|---------|-------------|----------|
| IT-NV-001 | Navigation accueil â†’ dÃ©mo | HIGH |
| IT-NV-002 | Navigation accueil â†’ donnÃ©es | HIGH |
| IT-NV-003 | Navigation dÃ©mo â†’ comparaison | HIGH |
| IT-NV-004 | Sidebar navigation | MEDIUM |

### 2.4 Tests d'intÃ©gration Classifier + UI

| Test ID | Description | PrioritÃ© |
|---------|-------------|----------|
| IT-CL-001 | Classification texte end-to-end | CRITICAL |
| IT-CL-002 | Classification image end-to-end | CRITICAL |
| IT-CL-003 | Changement de modÃ¨le | HIGH |
| IT-CL-004 | Comparaison 3 modÃ¨les simultanÃ©s | HIGH |

---

## ğŸ¤– 3. TESTS SPÃ‰CIFIQUES ML (Machine Learning Tests)

### 3.1 Tests de Performance du ModÃ¨le (Model Quality Gates)

| Test ID | MÃ©trique | Seuil minimum | PrioritÃ© |
|---------|----------|---------------|----------|
| ML-PF-001 | Accuracy globale | â‰¥ 75% | CRITICAL |
| ML-PF-002 | F1-Score macro | â‰¥ 70% | CRITICAL |
| ML-PF-003 | F1-Score weighted | â‰¥ 75% | HIGH |
| ML-PF-004 | Precision macro | â‰¥ 70% | HIGH |
| ML-PF-005 | Recall macro | â‰¥ 65% | HIGH |

**ImplÃ©mentation**:
```python
import pytest
from sklearn.metrics import accuracy_score, f1_score, classification_report

class TestModelPerformance:
    """Tests de qualitÃ© du modÃ¨le avec seuils minimaux."""

    ACCURACY_THRESHOLD = 0.75
    F1_MACRO_THRESHOLD = 0.70

    @pytest.fixture(scope="class")
    def model_predictions(self, test_dataset):
        """GÃ©nÃ¨re les prÃ©dictions sur le jeu de test."""
        clf = load_production_model()
        X_test, y_true = test_dataset
        y_pred = clf.predict(X_test)
        return y_true, y_pred

    def test_accuracy_above_threshold(self, model_predictions):
        """L'accuracy doit Ãªtre â‰¥ 75%."""
        y_true, y_pred = model_predictions
        accuracy = accuracy_score(y_true, y_pred)

        assert accuracy >= self.ACCURACY_THRESHOLD, \
            f"Accuracy {accuracy:.2%} below threshold {self.ACCURACY_THRESHOLD:.2%}"

    def test_f1_macro_above_threshold(self, model_predictions):
        """Le F1-score macro doit Ãªtre â‰¥ 70%."""
        y_true, y_pred = model_predictions
        f1 = f1_score(y_true, y_pred, average='macro')

        assert f1 >= self.F1_MACRO_THRESHOLD, \
            f"F1-macro {f1:.2%} below threshold {self.F1_MACRO_THRESHOLD:.2%}"
```

### 3.2 Tests de Non-RÃ©gression (Regression Tests)

| Test ID | Description | PrioritÃ© |
|---------|-------------|----------|
| ML-RG-001 | Accuracy ne baisse pas vs baseline | CRITICAL |
| ML-RG-002 | F1-score ne baisse pas | CRITICAL |
| ML-RG-003 | PrÃ©dictions identiques pour inputs fixes | HIGH |
| ML-RG-004 | Distribution des classes stable | MEDIUM |

**ImplÃ©mentation avec snapshot**:
```python
import json
from pathlib import Path

BASELINE_FILE = Path("tests/baselines/model_baseline.json")

class TestModelRegression:
    """Tests de non-rÃ©gression du modÃ¨le."""

    REGRESSION_TOLERANCE = 0.02  # 2% de tolÃ©rance

    @pytest.fixture
    def baseline_metrics(self):
        """Charge les mÃ©triques de rÃ©fÃ©rence."""
        with open(BASELINE_FILE) as f:
            return json.load(f)

    def test_no_accuracy_regression(self, current_metrics, baseline_metrics):
        """L'accuracy ne doit pas baisser de plus de 2%."""
        current = current_metrics["accuracy"]
        baseline = baseline_metrics["accuracy"]

        assert current >= baseline - self.REGRESSION_TOLERANCE, \
            f"Regression detected: {current:.2%} vs baseline {baseline:.2%}"

    def test_deterministic_predictions(self):
        """MÃªmes inputs = mÃªmes outputs (dÃ©terminisme)."""
        clf = DemoClassifier(seed=42)

        input_text = "Console PlayStation 5"
        result1 = clf.predict(text=input_text)
        result2 = clf.predict(text=input_text)

        assert result1.category == result2.category
        assert abs(result1.confidence - result2.confidence) < 0.001
```

### 3.3 Tests de Robustesse (Robustness Tests)

| Test ID | Cas de test | Comportement attendu | PrioritÃ© |
|---------|-------------|---------------------|----------|
| ML-RB-001 | Texte vide | Retourne prÃ©diction par dÃ©faut | HIGH |
| ML-RB-002 | Texte trÃ¨s long (10K chars) | Tronque et prÃ©dit | MEDIUM |
| ML-RB-003 | CaractÃ¨res spÃ©ciaux/emojis | Nettoie et prÃ©dit | MEDIUM |
| ML-RB-004 | Image corrompue | Erreur gracieuse | HIGH |
| ML-RB-005 | Image trÃ¨s petite (10x10) | Erreur ou upscale | MEDIUM |
| ML-RB-006 | Image trÃ¨s grande (8000x8000) | Redimensionne | MEDIUM |
| ML-RB-007 | Texte multilingue | Traduit et prÃ©dit | HIGH |
| ML-RB-008 | HTML malicieux | Sanitize et prÃ©dit | HIGH |

**ImplÃ©mentation**:
```python
class TestModelRobustness:
    """Tests de robustesse du modÃ¨le face aux edge cases."""

    @pytest.mark.parametrize("edge_case_text", [
        "",                              # Empty
        " " * 100,                       # Whitespace only
        "a" * 10000,                     # Very long
        "<script>alert('xss')</script>", # XSS attempt
        "ğŸ®ğŸ“±ğŸ’»ğŸ–¥ï¸",                      # Emojis only
        "ä»·æ ¼ä¾¿å®œè´¨é‡å¥½",                   # Chinese
        "Ğ¦ĞµĞ½a Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ°Ñ",                  # Russian
        None,                            # None value
    ])
    def test_handles_edge_case_text(self, edge_case_text):
        """Le modÃ¨le gÃ¨re les cas limites sans crash."""
        clf = DemoClassifier()

        # Ne doit pas lever d'exception
        try:
            result = clf.predict(text=edge_case_text or "")
            assert result is not None
            assert result.category in CATEGORY_MAPPING
        except ValueError:
            pass  # ValueError acceptable pour entrÃ©e invalide

    def test_handles_adversarial_input(self):
        """Test d'input adversarial."""
        clf = DemoClassifier()

        # Texte conÃ§u pour confondre le modÃ¨le
        adversarial = "livre console telephone jardin piscine figurine"
        result = clf.predict(text=adversarial)

        # Doit quand mÃªme retourner une prÃ©diction valide
        assert result.category in CATEGORY_MAPPING
        # Confiance devrait Ãªtre plus basse
        assert result.confidence < 0.95
```

### 3.4 Tests de Consistance Inter-ModÃ¨les

| Test ID | Description | PrioritÃ© |
|---------|-------------|----------|
| ML-CS-001 | 3 modÃ¨les texte donnent top-3 similaires | MEDIUM |
| ML-CS-002 | 3 modÃ¨les image donnent top-3 similaires | MEDIUM |
| ML-CS-003 | CorrÃ©lation des confidences entre modÃ¨les | LOW |

### 3.5 Tests de Performance (Latency/Throughput)

| Test ID | MÃ©trique | Seuil | PrioritÃ© |
|---------|----------|-------|----------|
| ML-LT-001 | Latence infÃ©rence texte | < 100ms | HIGH |
| ML-LT-002 | Latence infÃ©rence image | < 500ms | HIGH |
| ML-LT-003 | Throughput batch 100 | > 10/s | MEDIUM |
| ML-LT-004 | MÃ©moire max | < 2 GB | MEDIUM |

**ImplÃ©mentation**:
```python
import time
import pytest

class TestInferencePerformance:
    """Tests de performance d'infÃ©rence."""

    MAX_TEXT_LATENCY_MS = 100
    MAX_IMAGE_LATENCY_MS = 500

    def test_text_inference_latency(self):
        """L'infÃ©rence texte doit Ãªtre < 100ms."""
        clf = DemoClassifier()
        text = "Console PlayStation 5 nouvelle gÃ©nÃ©ration"

        # Warmup
        clf.predict(text=text)

        # Mesure
        start = time.perf_counter()
        for _ in range(100):
            clf.predict(text=text)
        elapsed = (time.perf_counter() - start) * 1000 / 100  # ms par infÃ©rence

        assert elapsed < self.MAX_TEXT_LATENCY_MS, \
            f"Text inference too slow: {elapsed:.1f}ms > {self.MAX_TEXT_LATENCY_MS}ms"

    @pytest.mark.slow
    def test_batch_throughput(self):
        """Le throughput batch doit Ãªtre > 10 prÃ©dictions/seconde."""
        clf = DemoClassifier()
        texts = [f"Product {i}" for i in range(100)]

        start = time.perf_counter()
        for text in texts:
            clf.predict(text=text)
        elapsed = time.perf_counter() - start

        throughput = len(texts) / elapsed
        assert throughput > 10, f"Throughput too low: {throughput:.1f}/s"
```

### 3.6 Tests de Data Quality (Validation des donnÃ©es)

| Test ID | Description | PrioritÃ© |
|---------|-------------|----------|
| ML-DQ-001 | Pas de NaN dans features | CRITICAL |
| ML-DQ-002 | Toutes les catÃ©gories prÃ©sentes | HIGH |
| ML-DQ-003 | Distribution des classes cohÃ©rente | MEDIUM |
| ML-DQ-004 | Pas de doublons dans test set | HIGH |
| ML-DQ-005 | Train/Test split correct (pas de fuite) | CRITICAL |

**ImplÃ©mentation avec Great Expectations style**:
```python
class TestDataQuality:
    """Tests de qualitÃ© des donnÃ©es."""

    def test_no_nan_in_features(self, training_data):
        """Aucune valeur NaN dans les features."""
        X, y = training_data
        assert not X.isnull().any().any(), "NaN values found in features"

    def test_all_categories_represented(self, training_data):
        """Les 27 catÃ©gories sont reprÃ©sentÃ©es."""
        X, y = training_data
        unique_categories = y['prdtypecode'].unique()

        assert len(unique_categories) == 27, \
            f"Expected 27 categories, found {len(unique_categories)}"

    def test_no_data_leakage(self, train_data, test_data):
        """Pas de fuite de donnÃ©es trainâ†’test."""
        train_ids = set(train_data.index)
        test_ids = set(test_data.index)

        overlap = train_ids & test_ids
        assert len(overlap) == 0, f"Data leakage: {len(overlap)} samples in both sets"
```

---

## ğŸŒ 4. TESTS END-TO-END (E2E Tests)

### 4.1 Tests Playwright (Browser Automation)

| Test ID | ScÃ©nario | PrioritÃ© |
|---------|----------|----------|
| E2E-001 | Parcours complet: accueil â†’ classification â†’ rÃ©sultat | CRITICAL |
| E2E-002 | Upload image et classification | HIGH |
| E2E-003 | Comparaison des 3 modÃ¨les | HIGH |
| E2E-004 | Navigation complÃ¨te (6 pages) | HIGH |
| E2E-005 | Galerie d'exemples | MEDIUM |
| E2E-006 | Export CSV distribution | LOW |

**ImplÃ©mentation**:
```python
from playwright.sync_api import Page, expect
import pytest

class TestE2EClassification:
    """Tests E2E avec Playwright."""

    BASE_URL = "http://localhost:8501"

    def test_full_classification_journey(self, page: Page):
        """Parcours utilisateur complet."""
        # 1. Page d'accueil
        page.goto(self.BASE_URL)
        expect(page.locator("text=Rakuten")).to_be_visible()

        # 2. Click sur "Classifier un Produit"
        page.click("button:has-text('Classifier')")
        page.wait_for_url("**/4_*DÃ©mo*")

        # 3. Remplir le formulaire
        page.fill("input[aria-label='DÃ©signation']", "iPhone 15 Pro Max")
        page.fill("textarea[aria-label='Description']", "Smartphone Apple")

        # 4. Classifier
        page.click("button:has-text('Classifier le Produit')")

        # 5. VÃ©rifier le rÃ©sultat
        expect(page.locator(".result-card")).to_be_visible(timeout=10000)
        expect(page.locator("text=CatÃ©gorie")).to_be_visible()

    def test_model_comparison(self, page: Page):
        """Test comparaison des modÃ¨les."""
        page.goto(f"{self.BASE_URL}/3_ğŸ§ _ModÃ¨les")

        # SÃ©lectionner mode texte
        page.click("text=ModÃ¨les Texte")

        # Remplir input
        page.fill("input", "Console PlayStation 5")

        # Lancer comparaison
        page.click("button:has-text('Comparer')")

        # VÃ©rifier 3 rÃ©sultats
        expect(page.locator(".model-card")).to_have_count(3)
```

### 4.2 Tests de Snapshot UI

```python
def test_home_page_snapshot(page: Page, assert_snapshot):
    """Snapshot de la page d'accueil."""
    page.goto(BASE_URL)
    page.wait_for_load_state("networkidle")

    assert_snapshot(page.screenshot(), "home_page.png")
```

---

## ğŸ”’ 5. TESTS DE SÃ‰CURITÃ‰ (Security Tests)

| Test ID | VulnÃ©rabilitÃ© | Test | PrioritÃ© |
|---------|--------------|------|----------|
| SEC-001 | XSS | Injection script dans input | HIGH |
| SEC-002 | Path Traversal | Upload fichier malicieux | HIGH |
| SEC-003 | DoS | Input trÃ¨s volumineux | MEDIUM |
| SEC-004 | Injection | CaractÃ¨res spÃ©ciaux SQL-like | LOW |

```python
class TestSecurity:
    """Tests de sÃ©curitÃ©."""

    XSS_PAYLOADS = [
        "<script>alert('xss')</script>",
        "<img src=x onerror=alert('xss')>",
        "javascript:alert('xss')",
        "<svg onload=alert('xss')>",
    ]

    @pytest.mark.parametrize("payload", XSS_PAYLOADS)
    def test_xss_prevention(self, payload):
        """Les payloads XSS sont sanitizÃ©s."""
        result = preprocess_product_text(payload, "")

        assert "<script>" not in result
        assert "javascript:" not in result
        assert "onerror=" not in result
```

---

## ğŸ“ 6. STRUCTURE DES FICHIERS DE TEST

```
tests/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ conftest.py                    # Fixtures partagÃ©es
â”œâ”€â”€ pytest.ini                     # Configuration pytest
â”‚
â”œâ”€â”€ unit/                          # Tests unitaires
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ test_mock_classifier.py
â”‚   â”œâ”€â”€ test_category_mapping.py
â”‚   â”œâ”€â”€ test_preprocessing.py
â”‚   â”œâ”€â”€ test_image_utils.py
â”‚   â”œâ”€â”€ test_data_loader.py
â”‚   â””â”€â”€ test_config.py
â”‚
â”œâ”€â”€ integration/                   # Tests d'intÃ©gration
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ test_page_loading.py
â”‚   â”œâ”€â”€ test_session_state.py
â”‚   â”œâ”€â”€ test_navigation.py
â”‚   â””â”€â”€ test_classification_flow.py
â”‚
â”œâ”€â”€ ml/                           # Tests ML spÃ©cifiques
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ test_model_performance.py
â”‚   â”œâ”€â”€ test_model_regression.py
â”‚   â”œâ”€â”€ test_model_robustness.py
â”‚   â”œâ”€â”€ test_inference_latency.py
â”‚   â””â”€â”€ test_data_quality.py
â”‚
â”œâ”€â”€ e2e/                          # Tests end-to-end
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ test_classification_journey.py
â”‚   â”œâ”€â”€ test_model_comparison.py
â”‚   â””â”€â”€ snapshots/                # Screenshots de rÃ©fÃ©rence
â”‚
â”œâ”€â”€ security/                     # Tests de sÃ©curitÃ©
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ test_input_sanitization.py
â”‚
â”œâ”€â”€ baselines/                    # DonnÃ©es de rÃ©fÃ©rence
â”‚   â”œâ”€â”€ model_baseline.json       # MÃ©triques baseline
â”‚   â””â”€â”€ golden_predictions.json   # PrÃ©dictions de rÃ©fÃ©rence
â”‚
â””â”€â”€ fixtures/                     # DonnÃ©es de test
    â”œâ”€â”€ sample_images/
    â”‚   â”œâ”€â”€ valid_product.jpg
    â”‚   â”œâ”€â”€ corrupted.jpg
    â”‚   â””â”€â”€ too_small.png
    â””â”€â”€ sample_texts/
        â””â”€â”€ test_products.csv
```

---

## âš™ï¸ 7. CONFIGURATION

### 7.1 `pytest.ini`

```ini
[pytest]
testpaths = tests
python_files = test_*.py
python_classes = Test*
python_functions = test_*

# Markers
markers =
    unit: Unit tests (fast)
    integration: Integration tests
    ml: Machine Learning specific tests
    e2e: End-to-end browser tests
    slow: Slow tests (>5s)
    security: Security tests

# Options par dÃ©faut
addopts =
    -v
    --tb=short
    --strict-markers
    -ra
    --cov=utils
    --cov=pages
    --cov-report=html:coverage_report
    --cov-report=term-missing
    --cov-fail-under=80

# Timeout par test
timeout = 30

# Parallel execution
# addopts = -n auto  # DÃ©commenter pour exÃ©cution parallÃ¨le
```

### 7.2 `conftest.py`

```python
"""
Fixtures partagÃ©es pour tous les tests.
"""
import pytest
import sys
from pathlib import Path
from PIL import Image
import pandas as pd
import io

# Ajouter le rÃ©pertoire source au path
sys.path.insert(0, str(Path(__file__).parent.parent / "src" / "streamlit"))

from utils.mock_classifier import DemoClassifier, TEXT_MODELS, IMAGE_MODELS
from utils.category_mapping import CATEGORY_MAPPING


# =============================================================================
# FIXTURES CLASSIFIER
# =============================================================================
@pytest.fixture
def demo_classifier():
    """Retourne un classifier initialisÃ©."""
    return DemoClassifier()


@pytest.fixture
def text_classifier():
    """Classifier configurÃ© pour le texte."""
    from utils.mock_classifier import TEXT_MODELS
    config = TEXT_MODELS["camembert"]
    return DemoClassifier(model_config=config)


@pytest.fixture
def image_classifier():
    """Classifier configurÃ© pour les images."""
    from utils.mock_classifier import IMAGE_MODELS
    config = IMAGE_MODELS["resnet50_svm"]
    return DemoClassifier(model_config=config)


# =============================================================================
# FIXTURES DONNÃ‰ES
# =============================================================================
@pytest.fixture
def sample_text():
    """Texte produit exemple."""
    return "Console PlayStation 5 nouvelle gÃ©nÃ©ration Sony"


@pytest.fixture
def sample_image():
    """Image produit exemple (100x100 rouge)."""
    img = Image.new('RGB', (100, 100), color='red')
    return img


@pytest.fixture
def sample_image_bytes():
    """Image en bytes pour simuler upload."""
    img = Image.new('RGB', (100, 100), color='blue')
    buffer = io.BytesIO()
    img.save(buffer, format='JPEG')
    buffer.seek(0)
    return buffer


@pytest.fixture
def all_category_codes():
    """Liste de tous les codes catÃ©gorie."""
    return list(CATEGORY_MAPPING.keys())


# =============================================================================
# FIXTURES DONNÃ‰ES ML
# =============================================================================
@pytest.fixture(scope="session")
def training_data():
    """Charge les donnÃ©es d'entraÃ®nement (cache session)."""
    # Ã€ adapter selon votre structure de donnÃ©es
    data_path = Path(__file__).parent.parent / "data"
    X_train = pd.read_csv(data_path / "X_train.csv")
    Y_train = pd.read_csv(data_path / "Y_train.csv")
    return X_train, Y_train


@pytest.fixture
def model_baseline():
    """Charge les mÃ©triques baseline."""
    baseline_path = Path(__file__).parent / "baselines" / "model_baseline.json"
    if baseline_path.exists():
        import json
        with open(baseline_path) as f:
            return json.load(f)
    return {
        "accuracy": 0.75,
        "f1_macro": 0.70,
        "f1_weighted": 0.75,
    }


# =============================================================================
# FIXTURES EDGE CASES
# =============================================================================
@pytest.fixture(params=[
    "",
    " " * 100,
    "a" * 10000,
    "<script>alert('xss')</script>",
    "ğŸ®ğŸ“±ğŸ’»",
    None,
])
def edge_case_text(request):
    """ParamÃ©trise les cas limites de texte."""
    return request.param


@pytest.fixture
def corrupted_image():
    """Retourne des bytes invalides (pas une image)."""
    return io.BytesIO(b"not an image content")


@pytest.fixture
def oversized_image():
    """Image trÃ¨s grande (8000x8000)."""
    img = Image.new('RGB', (8000, 8000), color='green')
    return img


# =============================================================================
# FIXTURES STREAMLIT
# =============================================================================
@pytest.fixture
def streamlit_app():
    """Initialise AppTest pour les tests Streamlit."""
    from streamlit.testing.v1 import AppTest
    return AppTest


# =============================================================================
# CONFIGURATION PLAYWRIGHT (E2E)
# =============================================================================
@pytest.fixture(scope="session")
def browser_context_args(browser_context_args):
    """Configuration du contexte navigateur."""
    return {
        **browser_context_args,
        "viewport": {"width": 1280, "height": 720},
        "locale": "fr-FR",
    }


# =============================================================================
# HELPERS
# =============================================================================
def pytest_configure(config):
    """Configuration pytest au dÃ©marrage."""
    # Ajouter des markers personnalisÃ©s
    config.addinivalue_line("markers", "gpu: tests requiring GPU")
    config.addinivalue_line("markers", "real_model: tests with real ML model")
```

---

## ğŸš€ 8. COMMANDES D'EXÃ‰CUTION

### 8.1 ExÃ©cution par type

```bash
# Tous les tests
pytest

# Tests unitaires uniquement (rapides)
pytest -m unit

# Tests d'intÃ©gration
pytest -m integration

# Tests ML
pytest -m ml

# Tests E2E (nÃ©cessite serveur Streamlit)
pytest -m e2e

# Tests de sÃ©curitÃ©
pytest -m security

# Exclure les tests lents
pytest -m "not slow"
```

### 8.2 Avec rapport de couverture

```bash
# HTML report
pytest --cov=. --cov-report=html

# Terminal report
pytest --cov=. --cov-report=term-missing

# Fail si couverture < 80%
pytest --cov=. --cov-fail-under=80
```

### 8.3 CI/CD Pipeline

```yaml
# .github/workflows/tests.yml
name: Tests

on: [push, pull_request]

jobs:
  test:
    runs-on: ubuntu-latest

    steps:
    - uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.10'

    - name: Install dependencies
      run: |
        pip install -r requirements.txt
        pip install -r requirements-test.txt

    - name: Run unit tests
      run: pytest -m unit --cov=. --cov-report=xml

    - name: Run integration tests
      run: pytest -m integration

    - name: Run ML tests
      run: pytest -m ml

    - name: Upload coverage
      uses: codecov/codecov-action@v3
      with:
        files: ./coverage.xml
```

---

## ğŸ“Š 9. MÃ‰TRIQUES DE QUALITÃ‰

### Dashboard de Tests

| MÃ©trique | Cible | Status |
|----------|-------|--------|
| Couverture code | â‰¥ 80% | ğŸ”´ |
| Tests unitaires passants | 100% | ğŸ”´ |
| Tests intÃ©gration passants | 100% | ğŸ”´ |
| Tests ML passants | 100% | ğŸ”´ |
| Temps d'exÃ©cution total | < 5 min | ğŸ”´ |
| Accuracy modÃ¨le | â‰¥ 75% | ğŸ”´ |
| F1-Score modÃ¨le | â‰¥ 70% | ğŸ”´ |

### Badges README

```markdown
![Tests](https://github.com/xxx/rakuten-classifier/actions/workflows/tests.yml/badge.svg)
![Coverage](https://codecov.io/gh/xxx/rakuten-classifier/branch/main/graph/badge.svg)
![Model Accuracy](https://img.shields.io/badge/accuracy-85%25-green)
```

---

## ğŸ¯ 10. PRIORISATION D'IMPLÃ‰MENTATION

### Phase 1: Foundation (Critique) - 2h
1. âœ… Structure des dossiers tests
2. âœ… `conftest.py` avec fixtures de base
3. âœ… Tests unitaires `mock_classifier.py`
4. âœ… Tests unitaires `category_mapping.py`

### Phase 2: Core (Haute prioritÃ©) - 3h
5. Tests unitaires `preprocessing.py`
6. Tests unitaires `image_utils.py`
7. Tests d'intÃ©gration chargement pages
8. Tests ML performance baseline

### Phase 3: Robustness (Moyenne prioritÃ©) - 2h
9. Tests de robustesse (edge cases)
10. Tests de non-rÃ©gression ML
11. Tests de latence infÃ©rence

### Phase 4: E2E & Polish (Si temps) - 2h
12. Tests E2E Playwright
13. Tests de sÃ©curitÃ©
14. Configuration CI/CD

---

## âœ… Checklist Soutenance

- [ ] â‰¥ 50 tests unitaires passants
- [ ] â‰¥ 10 tests d'intÃ©gration passants
- [ ] â‰¥ 5 tests ML spÃ©cifiques passants
- [ ] Couverture â‰¥ 80%
- [ ] Rapport HTML de couverture gÃ©nÃ©rÃ©
- [ ] Temps d'exÃ©cution < 2 minutes
- [ ] Badge de tests dans README
- [ ] DÃ©monstration live `pytest -v` pendant soutenance

---

*Document crÃ©Ã© le 16 janvier 2025 - StratÃ©gie de Tests Enterprise-Grade*
